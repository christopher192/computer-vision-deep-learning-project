{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abea27cd",
   "metadata": {},
   "source": [
    "## Object Detection Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3478ae",
   "metadata": {},
   "source": [
    "### 1. Intersection and Union\n",
    "\n",
    "<img src=\"image/iou.png\" alt=\"iou\" style=\"width: 500px; float: left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab601ef",
   "metadata": {},
   "source": [
    "<p>The IOU (Intersection over Union) metric is used to evaluate the overlap between two bounding boxes, requiring two essential elements: the ground truth bounding box and the predicted bounding box. IOU is computed as the ratio of the area of their overlap to the area of union. To evaluate detection, we need to set threshold called IoU threshold which determine how well localization works. If IoU > Threshold = Prediction intersects well with the ground truth.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21419fd",
   "metadata": {},
   "source": [
    "### 2. Confidence Score\n",
    "\n",
    "<img src=\"image/confidence score.png\" alt=\"confidence score\" style=\"width: 500px; float: left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698147f",
   "metadata": {},
   "source": [
    "<p>Probability that bounding box contains object, usually predicted by classifier. Orange box shows good overlap but low confidence score. That is because classifier think that the object inside the bounding box is different object. Blue box shows a bounding box with reasonable overlap and high confidence score. We need to set threshold for confidence score called confidence threshold which refer to the probability of finding an object inside the bounding box.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178a21f",
   "metadata": {},
   "source": [
    "### 3. Confusion Matrix\n",
    "\n",
    "True positive: `IoU > IoU threshold` and `confidence score > confidence threshold`\n",
    "<br />\n",
    "False positive: `No ground truth bounding box (IoU = 0)` or `IoU < IoU threshold` and `confidence score > confidence threshold`\n",
    "<br />\n",
    "`IoU > IoU threshold`, `no bounding box detected near ground truth box` and `confidence score < confidence threshold`\n",
    "<br />\n",
    "False negative: `IoU < IoU threshold`, `no bounding box detected near ground truth box/ Not enough overlap` and `confidence score > confidence threshold`\n",
    "<br />\n",
    "False negative: `IoU > IoU threshold` and `confidence score < confidence threshold`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede8346",
   "metadata": {},
   "source": [
    "### Mean Average Precision (mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655cf4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef538e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sample for ground truth, predicted box and confidence score\n",
    "ground_truth_box = torch.tensor([[8, 12, 352, 498], [10, 15, 450, 500]], dtype = torch.float)\n",
    "\n",
    "predicted_box = torch.tensor([\n",
    "    [1.000000, 116.384613, 353.000000, 116.384613], [1.000000, 1.000000, 353.000000, 500.000000],\n",
    "    [9.000000, 14.000000, 452.500000, 500.500000], [1.000000, 154.846161, 353.000000, 154.846161],\n",
    "    [196.776474, 231.769226, 196.776474, 231.769226], [1.000000, 116.384613, 353.000000, 116.384613],\n",
    "    [1.000000, 1.000000, 353.000000, 500.000000], [1.000000, 231.769226, 353.000000, 231.769226],\n",
    "    [45.000000, 235.230774, 175.000000, 361.230774], [44.500000, 234.230774, 176.000000, 362.230774]\n",
    "], dtype = torch.float)\n",
    "\n",
    "confidence_score = torch.tensor([\n",
    "    0.180000, 0.985000, 0.98000, 0.200000, 0.090000, 0.170000, 0.570000, 0.210000, 0.855000, 0.850000\n",
    "], dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e00cec",
   "metadata": {},
   "source": [
    "`cv2.rectangle` -> `start point (xmin, ymin), (left, top)`, `end point (xmax, ymax), (right, bottom)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5bc01f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization of ground truth\n",
    "# Set random image with width 600 and height 600\n",
    "image = np.ones((600, 600, 3), dtype = np.uint8) * 255\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "cv2.rectangle(image_rgb, (8, 12), (352, 498), \n",
    "                  color = (0, 255, 0), thickness = 2)\n",
    "\n",
    "cv2.rectangle(image_rgb, (10, 15), (450, 500), \n",
    "                  color = (0, 255, 0), thickness = 2)\n",
    "\n",
    "cv2.circle(image_rgb, (8, 12), radius = 0, color = (0, 0, 255), thickness = 15)\n",
    "cv2.circle(image_rgb, (352, 498), radius = 0, color = (0, 0, 255), thickness = 15)\n",
    "\n",
    "cv2.circle(image_rgb, (10, 15), radius = 0, color = (0, 0, 255), thickness = 15)\n",
    "cv2.circle(image_rgb, (450, 500), radius = 0, color = (0, 0, 255), thickness = 15)\n",
    "\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d0af58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization of ground truth with predicted box\n",
    "sample_predicted_box = predicted_box[2]\n",
    "sample_xmin, sample_ymin, sample_xmax, sample_ymax = int(torch.round(sample_predicted_box[0]).to(torch.int)), int(torch.round(sample_predicted_box[1]).to(torch.int)), \\\n",
    "    int(torch.round(sample_predicted_box[2]).to(torch.int)), int(torch.round(sample_predicted_box[3]).to(torch.int))\n",
    "\n",
    "image = np.ones((600, 600, 3), dtype = np.uint8) * 255\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "cv2.rectangle(image_rgb, (8, 12), (352, 498), \n",
    "                  color = (0, 255, 0), thickness = 2)\n",
    "\n",
    "cv2.rectangle(image_rgb, (10, 15), (450, 500), \n",
    "                  color = (0, 255, 0), thickness = 2)\n",
    "\n",
    "cv2.circle(image_rgb, (8, 12), radius = 0, color = (0, 0, 255), thickness = 15)\n",
    "cv2.circle(image_rgb, (352, 498), radius = 0, color = (0, 0, 255), thickness = 15)\n",
    "\n",
    "cv2.circle(image_rgb, (10, 15), radius = 0, color = (0, 0, 255), thickness = 15)\n",
    "cv2.circle(image_rgb, (450, 500), radius = 0, color = (0, 0, 255), thickness = 15)\n",
    "\n",
    "cv2.rectangle(image_rgb, (sample_xmin, sample_ymin), (sample_xmax, sample_ymax), \n",
    "                  color = (255, 0, 0), thickness = 2)\n",
    "\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232bcc90",
   "metadata": {},
   "source": [
    "#### IoU calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "293f875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([False,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([False,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([False,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([False,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([False,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([False, False,  True, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n"
     ]
    }
   ],
   "source": [
    "# Sort based on confidence score\n",
    "sorted_index = np.argsort(-confidence_score)\n",
    "sorted_predicted_box = predicted_box[sorted_index, :]\n",
    "\n",
    "# Initialize true/ false positive\n",
    "true_positive = torch.zeros((len(sorted_index)), dtype = torch.float64)\n",
    "false_positive = torch.zeros((len(sorted_index)), dtype = torch.float64)\n",
    "\n",
    "# If there are not ground true box\n",
    "if len(ground_truth_box) == 0:\n",
    "    false_positive = torch.ones((len(sorted_index)), dtype = torch.float64)\n",
    "# If there are ground true box\n",
    "else:\n",
    "    true_positive = torch.zeros((len(sorted_index)), dtype = torch.float64)\n",
    "    false_positive = torch.zeros((len(sorted_index)), dtype = torch.float64)\n",
    "    is_obj_already_detected = [False for _ in range(ground_truth_box.shape[0])]\n",
    "    intersection_threshold = 0.5\n",
    "    \n",
    "    for box_number in range(len(sorted_index)):\n",
    "        get_predicted_box = predicted_box[box_number, :]\n",
    "\n",
    "        ixmin = torch.max(ground_truth_box[:, 0], get_predicted_box[0])\n",
    "        iymin = torch.max(ground_truth_box[:, 1], get_predicted_box[1])\n",
    "        ixmax = torch.min(ground_truth_box[:, 2], get_predicted_box[2])\n",
    "        iymax = torch.min(ground_truth_box[:, 3], get_predicted_box[3])\n",
    "        \n",
    "        width = torch.max(ixmax - ixmin + 1., torch.tensor(0.))\n",
    "        height = torch.max(iymax - iymin + 1., torch.tensor(0.))\n",
    "        \n",
    "        intersection_area = width * height\n",
    "        \n",
    "        union = ((get_predicted_box[2] - get_predicted_box[0] + 1.) * (get_predicted_box[3] - get_predicted_box[1] + 1.) +\n",
    "                 (ground_truth_box[:, 2] - ground_truth_box[:, 0] + 1.) *\n",
    "                 (ground_truth_box[:, 3] - ground_truth_box[:, 1] + 1.) - intersection_area)\n",
    "        \n",
    "        intersection_over_union = intersection_area / union\n",
    "\n",
    "        is_pass_threshold, max_iou_index = torch.max(intersection_over_union) >= intersection_threshold, \\\n",
    "            torch.argmax(intersection_over_union)\n",
    "        \n",
    "        if is_pass_threshold and not is_obj_already_detected[max_iou_index]:\n",
    "            true_positive[box_number] = 1\n",
    "            is_obj_already_detected[max_iou_index] = True\n",
    "        else:\n",
    "            false_positive[box_number] = 1\n",
    "            \n",
    "    true_positive_list = torch.cumsum(true_positive, dim = 0)\n",
    "    false_positive_list = torch.cumsum(false_positive, dim = 0)\n",
    "    false_negative_list = ground_truth_box.shape[0] - true_positive\n",
    "    \n",
    "    precision = true_positive_list / (false_positive_list + true_positive_list + torch.finfo(torch.float64).eps)\n",
    "    recall = true_positive_list / (true_positive_list + false_negative_list + torch.finfo(torch.float64).eps)\n",
    "\n",
    "    average_precision = torch.tensor(0.)\n",
    "    \n",
    "    if precision is None or recall is None:\n",
    "        average_precision = torch.tensor(np.nan)\n",
    "        \n",
    "    recall_level = torch.linspace(0, 1, 11)\n",
    "    \n",
    "    for level in recall_level:\n",
    "        recall_check = recall >= level\n",
    "        print(recall_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d533f7f",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "https://medium.com/@shroffmegha6695/object-detection-with-deep-learning-beginners-friendly-key-terms-explanation-d4fb594fea83#:~:text=Confidence%20score%20%3A%20A%20score%20which,the%20ground%20truth%20bounding%20box."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
